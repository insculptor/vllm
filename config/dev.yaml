dirs:
  ROOT_DIR: "/home/ravi/Desktop/KB/projects/vllm"
  MODELS_BASE_DIR: "/home/ravi/Desktop/KB/projects/MODELS"
  CACHE_DIR: "/home/ravi/Desktop/KB/projects/vllm/cache"
  VECTORSTORE_BASE_DIR: "/home/ravi/Desktop/KB/projects/vllm/vectorstore"
  LOG_DIR: "/home/ravi/Desktop/KB/projects/vllm/logs/"


## Models
models:
  CAUSAL_MODEL: "facebook/bart-large-cnn" #"meta-llama/Llama-3.2-3B-Instruct"
  EMBEDDING_MODEL: "/home/ravi/Desktop/KB/projects/MODELS/WhereIsAI/UAE-Large-V1"
  RERANKER_MODEL: "/home/ravi/Desktop/KB/projects/MODELS/mixedbread-ai/mxbai-rerank-large-v1"
  SUMMARIZATION_MODEL: "/home/ravi/Desktop/KB/projects/MODELS/facebook/bart-large-cnn"

## vLLM Args
sampling_parameters:
  temperature: 0.8
  top_p: 0.95
  max_tokens: 100

engine_args:
  model: "/home/ravi/Desktop/KB/projects/MODELS/meta-llama/Llama-3.2-3B-Instruct"
  load_format: "safetensors"
  tensor_parallel_size: 1
  max_num_batched_tokens: 8192
  dtype: "float16"
  gpu_memory_utilization: 0.9
  max_model_len: 8192
  swap_space: 4
  trust_remote_code: True  # Allow trust remote code
  tokenizer_mode: "auto"
  device: "auto"


## Vector Database
vectordb:
  FAISS_L2_INDEX: "faiss_l2.index"
  EMBEDDING_DIM: 1024
  CHUNK_SIZE: 350
  CHUNK_OVERLAP: 50
  MEGA_CHUNK_MULTIPLIER: 10
  TOP_N: 10 # Documents fetched from VectorDB
  TOP_K: 2 # Documents returned after reranking


summarize:
  MAX_LENGTH: 350 ## Same as Chunk Size
  MIN_LENGTH: 50
  LENGTH_PENALTY: 2.0
  NUM_BEAMS: 4
  EARLY_STOPPING: True

mongodb:
  HOST: ['localhost', 'localhost2', 'localhost3']
  PORT: 27017
  DB_NAME: 'your_database_name'
  COLLECTION: 'your_collection_name'


## Serving
apiserver:
  host: "0.0.0.0"
  vllm_port: 8000
  models_port: 8001
  timeout_keep_alive: 5  # seconds

logger:
  level: "debug"
  models_log_file: "models_engine.log"
  vllm_log_file: "vllm_engine.log"
